# -*- coding: utf-8 -*-
"""Mistral7B-MedMCQA-QLoRA-unsloth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFg766HRP4_Wr05spQspeBfHTdO1vgCm
"""

# !pip install torch==2.9.0

# !pip install -q unsloth

# from google.colab import drive
# drive.mount("/content/drive")

from data_loader import train_data, instruction_tuned_dataset, valid_data
from model_loader import load_model, load_model_for_inference
from trainer import define_trainer
from inference import calculate_acc

from huggingface_hub import notebook_login
notebook_login()

# python library for Weights & Biases API
import wandb
wandb.login()

max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

data = train_data('data/train.json')
dataset = instruction_tuned_dataset(data)

model, tokenizer = load_model(max_seq_length, dtype, load_in_4bit)
trainer = define_trainer(model, tokenizer, dataset, max_seq_length)
trainer_stats = trainer.train()
trainer.push_to_hub()

val_data = valid_data()
model, tokenizer = load_model_for_inference(max_seq_length, dtype, load_in_4bit)
accuracy = calculate_acc(val_data, model, tokenizer)
print(f'Accuracy : {accuracy}')